# 评论&笔记

- [Transformer](#202502021731)
- [BERT](#202502021732)
- [ViT](#202502021733)
- [MAE](#202502021734)
- [MoCo](#202502021735)
- [Swin Transformer](#202502021736)
- [CLIP](#202502021737)
- [Codex](#202502021738)
- [AlphaCode](#202502021739)
- [GPT 1 2 3](#202502021740)
- [InstructGPT](#202502021741)
- [Claude](#202502021742)
- [Llama 3](#202502021743)
- [Whisper](#202502021744)
- [DALL-E 1 2 3](#202502021745)
- [AlphaFold 1 2 3](#202502021746)
- [ViLT](#202502021747)
- [ALBEF](#202502021748)
- [VLMo](#202502021749)
- [BLIP](#202502021750)
- [CoCa](#202502021751)
- [BeiTv](#202502021752)
- [Movie Gen](#202502021753)
- [HunyuanVideo](#202502021754)

## <span id="202502021731"> Attention Is All You Need </span>

## <span id="202502021732"> BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding </span>

## <span id="202502021733"> An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale </span>

## <span id="202502021734"> Masked Autoencoders Are Scalable Vision Learners </span>

## <span id="202502021735"> Momentum Contrast for Unsupervised Visual Representation Learning </span>

## <span id="202502021736"> Swin Transformer: Hierarchical Vision Transformer using Shifted Windows </span>

## <span id="202502021737"> Learning Transferable Visual Models From Natural Language Supervision </span>

## <span id="202502021738"> Evaluating Large Language Models Trained on Code </span>

## <span id="202502021739"> Competition-Level Code Generation with AlphaCode </span>

## <span id="202502021740"> GPT 1 2 3 </span>

## <span id="202502021741"> Training language models to follow instructions with human feedback </span>

## <span id="202502021742"> Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback </span>

## <span id="202502021743"> The Llama 3 Herd of Models </span>

## <span id="202502021744"> Robust Speech Recognition via Large-Scale Weak Supervision </span>

## <span id="202502021745"> DALL-E 1 2 3 </span>

## <span id="202502021746"> AlphaFold 1 2 3 </span>

## <span id="202502021747"> ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision </span>

## <span id="202502021748"> Align before Fuse: Vision and Language Representation Learning with Momentum Distillation </span>

## <span id="202502021749"> VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts </span>

## <span id="202502021750"> BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation </span>

## <span id="202502021751"> CoCa: Contrastive Captioners are Image-Text Foundation Models </span>

## <span id="202502021752"> Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks </span>

## <span id="202502021753"> Movie Gen: A Cast of Media Foundation Models </span>

## <span id="202502021754"> HunyuanVideo: A Systematic Framework For Large Video Generative Models </span>